## Lasso and Ridge Regression Using GLMNET Library

```{r}

# Install and load the glmnet package
# install.packages("glmnet")
library(glmnet)

# Generate some sample data
set.seed(123)
x <- matrix(rnorm(100 * 20), 100, 20)
y <- rnorm(100)

# Split the data into training and testing sets
train_indices <- sample(1:100, 80)
x_train <- x[train_indices, ]
y_train <- y[train_indices]
x_test <- x[-train_indices, ]
y_test <- y[-train_indices]

# Standardize the predictors
x_train <- scale(x_train)
x_test <- scale(x_test)

# Ridge Regression
alpha_ridge <- 0  # 0 for Ridge regression
ridge_model <- cv.glmnet(x_train, y_train, alpha = alpha_ridge)

# Lasso Regression
alpha_lasso <- 1  # 1 for Lasso regression
lasso_model <- cv.glmnet(x_train, y_train, alpha = alpha_lasso)

# Predictions
ridge_pred <- predict(ridge_model, newx = x_test)
lasso_pred <- predict(lasso_model, newx = x_test)

# Evaluate performance on the test set (you can use different metrics based on your problem)
mse_ridge <- mean((ridge_pred - y_test)^2)
mse_lasso <- mean((lasso_pred - y_test)^2)

# Display results
cat("Ridge Regression MSE:", mse_ridge, "\n")
cat("Lasso Regression MSE:", mse_lasso, "\n")

# Display selected features for Lasso
lasso_features <- coef(lasso_model, s = "lambda.min")
selected_features <- which(lasso_features != 0)
cat("Selected Features for Lasso:", selected_features, "\n")

```

## Ridge From Scratch
```{r}
# Function to perform Ridge Regression using Gradient Descent
ridge_regression_gradient_descent <- function(X, y, alpha = 1, learning_rate = 0.01, epochs = 1000) {
  n <- nrow(X)
  p <- ncol(X)
  w <- rep(0, p)  # Initialize weights
  
  for (epoch in 1:epochs) {
    # Predictions
    y_pred <- X %*% w
    
    # Update weights using gradient descent with Ridge penalty
    gradient <- (t(X) %*% (y_pred - y) + alpha * w) / n
    w <- w - learning_rate * gradient
    
    # Display progress every 100 epochs
    if (epoch %% 100 == 0) {
      cost <- mean((y_pred - y)^2) + alpha * sum(w^2)
      cat("Epoch:", epoch, "  Cost:", cost, "\n")
    }
  }
  
  return(w)
}

# Example usage:
set.seed(123)
X <- matrix(rnorm(100 * 5), 100, 5)
y <- X[,1] + 2 * X[,2] + rnorm(100)
ridge_weights <- ridge_regression_gradient_descent(X, y, alpha = 0.1)
cat("Ridge Regression Weights:", ridge_weights, "\n")


```

## Lasso From Scratch
```{r}
# Install and load necessary packages
# install.packages("ggplot2")
library(ggplot2)

# Function to generate synthetic data
generate_data <- function(n, noise_sd = 0.5) {
  set.seed(123)
  x <- seq(0, 4, length.out = n)
  y <- 3 * x + sin(2 * pi * x) + rnorm(n, sd = noise_sd)
  return(data.frame(x = x, y = y))
}

# Function to fit polynomial regression models
fit_polynomial <- function(data, degree) {
  formula <- as.formula(paste("y ~ poly(x, degree = ", degree, ")"))
  model <- lm(formula, data = data)
  return(model)
}

# Function to plot the data and regression model
plot_regression <- function(data, model, title) {
  ggplot(data, aes(x, y)) +
    geom_point(color = "blue") +
    geom_line(aes(y = predict(model, newdata = data)), color = "red") +
    ggtitle(title) +
    theme_minimal()
}

# Generate synthetic data
data <- generate_data(50)

# Set different polynomial degrees to test
degrees <- c(1, 3, 5, 10)

# Plot regression models of different degrees
plots <- lapply(degrees, function(degree) {
  model <- fit_polynomial(data, degree)
  title <- paste("Polynomial Degree =", degree)
  plot_regression(data, model, title)
})

# Display plots side by side
library(gridExtra)
grid.arrange(grobs = plots, ncol = 2)

```